---
layout: post
title: Text Classification:BiLSTM BCN+ELMo and BERT on three datasets
---

Text classification as an important task in natural lanugage understanding (NLP) has been widely studied over the last several decades. The task describes input as a document and output as the category of which the document belongs to. In literature, both supervised and unsupervised methods have been applied for text classification. This blog only sets its scope on supervised learning methods. More specifically, it only focues on deep learning supervised methods that have gained great success in recent years. For knowing about latest advances of unsupervised methods in text classification, here is a recommended recently-published paper ([Haj-Yahia, et al., 2019](https://www.aclweb.org/anthology/P19-1036.pdf)).

<!-- In terms of supervised methods, Earlier, trandional machine learning techniques were mainstream for text classification. These methods normally work by extracting features that are perceived informative of 
 -->

This blog amis to use three different deep leanring methods for text classification on three datasets. The code repository associated with this blog can be found [here](code.com). Three datasets used in the blog are [5AbstractsGroup](https://github.com/qianliu0708/5AbstractsGroup), [SST-2](https://nlp.stanford.edu/sentiment/treebank.html) and [reddit_jokes](https://github.com/taivop/joke-dataset). These datasets are conditioned under license so you need to download them through their original sources.


The structure of the blog is organized as follows. 


## Table of Contents


- Datasets description
- Bi-LSTM and experiment
- BCN+ELMo and experiment
- BERT and experiment
- Experimental results
- Conclusion


#### 1. Datasets description

Table 1 gives details of the three datasets that are used in the blog. As seen from the table, the three datasets are different in types, including topic, sentiment and joke. They are also different in size, ranging from 6k to 155k. The diverse features make it more robust later on for experimental comparions between different methods.


![_config.yml]({{ site.baseurl }}/images/text_class_three/stats_datasets.png)


- [5AbstractsGroup](https://github.com/qianliu0708/5AbstractsGroup)	is a topic-based datasets. It consists of 6246 examples. Indeed, this is not a big dataset for training a deep learning model. The examples are extracted from Web of Science and are a collection of academic papers with five different topics - business, artificial intelligence, sociology, transport and law. In this blog, I only use abstracts of these papers as the training texts. The dataset is split into train/dev/test: 4996/624/626.


- [SST-2](https://nlp.stanford.edu/sentiment/treebank.html) refers to Standford Sentiment Treeback 2 and it is a famous dataset in sentiment analysis. It consists of short text sequences that are manually anotated in terms of their sentiment polarity, namely, positive (1) or negative (0). This dataset is relatively larger than  5AbstractsGroup with 68220 examples. The standard dataset from [the official website](https://nlp.stanford.edu/sentiment/treebank.html) contains a training set (67349 examples) and a development set (871 examples). I keep the scenario so I can compare my three methods to the state-of-the-art performance on development set (can be seen from [GLUE benchmark leaderboard](https://gluebenchmark.com/leaderboard)).

- [reddit_jokes](https://github.com/taivop/joke-dataset) is a collection of English plaintext jokes scraped from Reddit. Originally, it has 195k examples and each joke is assigned a post score indicating the joke's popularity or to say the extent of humor. I removed jokes with long texts and only keep those with words less than 50. Also, the title and body fields in the orignal datsets are combined to a single content field with <t/b> as separator. Because orginally the jokes are represented by a post score that is not suitable for text classification. I transformed the numeric scores to four categorical classes indicating four different levels of humor. The four classes are low, medium, high and crazy that are generated by setting up a score range. A joke that falls in a higher score range is assigned to a higher level of humor. After the preprocessing, 155027 examples are kept and subsequently split to train/dev/test: 124021/15502/15504.

#### 2. Bi-LSTM and experiment

This section presents a popular recurrent nerual network (RNN) method - Bidirectional Long-term Short Memory (Bi-LSTM) for classification on the three datasets. LSTM is memory-based unit in RNN, which helps to summarize the state of previous t-1 tokens in a sequence given a token at time step t. For example, Figure 1 (a) shows the process of LSTM in RNN (picture credit: [Zhang, et al., 2018](https://arxiv.org/ftp/arxiv/papers/1801/1801.07883.pdf)). At time step t, a token w can be represented by a vector **<a href="https://www.codecogs.com/eqnedit.php?latex=x_{t}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_{t}" title="x_{t}" /></a>** through an embedding layer. The LSTM unit takes **<a href="https://www.codecogs.com/eqnedit.php?latex=x_{t}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x_{t}" title="x_{t}" /></a>** with previous t-1 hidden states as input to output a new hidden state that summarizes important information of the sequence so far (from time step 0 to t). It summarizes important information by remembering or forgetting information through the recurrent process. It ends up with a vector that summarizes the whole sequence at the last time step. The last vector is normally used as representation input of subsequent feedforward layers for generating probabilities distribution over all classes. Unlike LSTM only summarizing information in one direction from left to right, Bi-LSTM sees context of a token at time step t in both left-to-right and right-to-left directions (as you can see from Figure 1 (b)). To better understand how LSTM/Bi-LSTM works, one point worth bearing in mind is that, in the recurrent process, all stuff are represented by vectors so as complicated mathematics functions can be leveraged on.

![_config.yml]({{ site.baseurl }}/images/text_class_three/lstm_in_rnn.png)


If you are not so familar with LSTM, here is a recommended roadmap for learning LSTM.

1. [Know basics of neural network](http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-backpropagation/) including computational graphs, perceptrons, training criterion, and activication functions, optimazition and back propagation, etc.

2. Definitely have a look at [colah's blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) on understanding LSTM Networks where you find excitingly how mathematics leverage "memory" on neural network

3. Next, you need to go deeper into implementation level. Although there are many open souce library that you can rely on to implement LSTM in real-world applications. For me, I am interested in designing deep learning models for NLP tasks, so [AllenNLP](https://github.com/allenai/allennlp) is highly recommended if you have the same interest. With AllenNLP, you will easily understand how matrix is transformed in a neural network through breakpoint debugging. Not only this benefit, this library is specifically developed for NLP researchers. The beautiful design of the lib also enables to built up a custom model flexiblely like lego stacking. 

4. Paper reading for research interests

Back to the topic, the following graph presents the architecture of a simple Bi-LSTM model that are trained on the datasets. It embeds the text first (through [glove.6B.100d](https://nlp.stanford.edu/projects/glove/)) and then encode it with a Bi-LSTM Encoder, and then pass the result through a feedforward network, the output is used as the scores for each label. The model's structure is designed with reference to AllenNLP's built-in [basic classifier](https://github.com/allenai/allennlp/blob/master/allennlp/models/basic_classifier.py). 


![_config.yml]({{ site.baseurl }}/images/text_class_three/lstm_structure.png)


In terms of experimental setup, examples are processed in batch of 64 and trained in 40 epochs with patience=5 for early stopping based on accuracy evaluated on validation set. Adam is used for optimizer and cross entropy is used as the objective function. The experiment is run on a single nvidia RTX 2060 GPU. With this GPU configuration and simplicity of the Bi-LSTM model, any one of the three experiments for each dataset is run no more than 3 minutes.

#### 3. BCN+ELMo and experiment

#### 4. BERT and experiment

#### 5. Experimental results


#### 6. Conclusion

<!-- Three methods are Bi-LSTM, BCN+ELMo and BERT. -->



If you have any doubts or any my mistakes you found in the post, send me an email via [wangcongcongcc@gmail.com](mailto:wangcongcongcc@gmail.com) or you are welcome to talk with me about any NLP relevant questions through [my Twitter account](https://twitter.com/WangcongcongCC).

 