---
layout: post
title: Transection - Transformers for English to Chinese Translation
---

This post presents how to train a sequence to sequence **Trans**former model for **E**nglish to **C**hinese transla**tion**, nicely abbreviated to Transection. We adopt BART's ([Lewis, Mike, et al. 2019](https://arxiv.org/abs/1910.13461)) architecture for this model and train it in two ways. The first way is to train it from scratch and second way is to fine-tune it from BART's pre-trained [base checkpoint](https://huggingface.co/facebook/bart-base) that is available in ðŸ¤—transformers. [The training examples](https://github.com/brightmart/nlp_chinese_corpus) consist of around 5M English-Chinese sequence pairs, along with a test set that has around 40k pairs. Later in this blog, the performance on the test set measured by [sacrebleu](https://github.com/mjpost/sacrebleu) is compared between the two ways. In addition, a popular pre-trained model in this domain, i.e., [`Helsinki-NLP/opus-mt-en-zh`](https://huggingface.co/Helsinki-NLP/opus-mt-en-zh) from ðŸ¤—Huggingface's models hub is used as a baseline to the two ways. Not only in sacrebleu, their performance in other metrics such as generalisation, model size, and training cost is also discussed.


TL;DR: For quick access to the outcomes of this project without reading through this post, here is [the code repository](#) and [a demo site](#). If you want to know more what is behind and how the model is trained, then it is better to have a look the blog that is organised as follows.

<!--
1. [Background](#section_1)
2. [Modle Architecture - Why BART?](#section_2)
3. [Data Preparation](#section_3)
4. [From Scratch or Fine-tuning](#section_4)
5. [Evaluation and Discussion](#section_5)
6. [Future Work](#section_6)-->


<!-- ### 1. Background
<span id="section_1"></span>
 -->
<!-- This will be something about the translation model and its linking to language model. I do not like to be official and research paper like but just want it to be casual so I prefer to use "I" in this paper. My motivation is to express what I know in a free way. This may benefit someone who are seeking for understanding the practical part of the translation task. Believe it or not, that is truely what I want to share and try the best to make every word in this blog as correct as possible. Yeah, maybe a storyingtelling to attract people's interest into this task. storyingtelling is what i like the most and feel comfortable. Whenever it is necessay, I switch between objective truth stating and subjective narrative (using the word "I"). -->

<!-- 
### 2. Modle Architecture - Why BART?
<span id="section_2"></span> -->

<!-- I considered many alternatives. Here is the story. Taking a peek at the model summary page in transformers, there are a number of sequence-to-sequence models that suits well the problem described here. The first option came to my mind is T5. However, I gave it up after trying the following commands. 
This says the default T5 tokenizer does not embed Chinese tokens into its vocabulary. Hence, in order to train a T5-like models in this task, the tokenizer (T5 uses SentencePiece tokenization) has to be pre-trained. Although this additional tokenizer pre-training can be easily achieved by leveraging the official sentencepiece library, fine-tuning T5's pre-trained weights is still hard. This is mainly because the newly pre-trained vocabulary will change the T5's pre-trained token embeddings that were pre-trained based on the original vocabulary. In short, there are quite some extra steps to take if using T5 for this task. Then the question comes to why not mBART? The answer is simple, the smallest pre-trained mBART that can be found from here is `mBART-base` that has around 770M parameters. This is too large for a personal-use accelaration device such as GPU. Since I just got a 8GB 2070 Super GPU that is kindly offered by one of my friends, I simply gave it up for computational resources concern. Having eliminating these candidates, finally, I found BART is a good choice.

 -->


### More resources.

* [The video inspiring the project](https://www.youtube.com/watch?v=fZSFNUT6iY8)
* [Some commercial products leveraging GPT-2 for auto code completion](https://kite.com/integrations/kite-vs-tabnine/)
* [Related Paper - Code Generation as a Dual Task of Code Summarization](https://papers.nips.cc/paper/8883-code-generation-as-a-dual-task-of-code-summarization.pdf)


Disclaimer: The introduction to Autocoder is by no means showing it generates codes with a good sense of reasoning. More of this blog's purpose is to introduce the state-of-the-art generative model GPT-2 and present an example of how it is used for code completion.

Last, thanks to [JiaZheng](https://jiazhengli.com/) who kindly provided the computation resources for making this blog possible. 

If you have any thoughts you'd like to share with me about the blog, send me an email via [wangcongcongcc@gmail.com](mailto:wangcongcongcc@gmail.com) or you are welcome to talk with me on [my Twitter](https://twitter.com/WangcongcongCC).

